# -*- coding: utf-8 -*-
"""Web Scraping the Udacity Course List.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ESSYb-T0PzavoqcAdLoldS--MhUzTZ6b
"""

from bs4 import BeautifulSoup
import requests

response = requests.get("https://www.udacity.com/courses/all")
html_soup = BeautifulSoup(response.content, 'html.parser')

#find all the URLs (items in the html where href exists)
url = html_soup.find_all(href=True)

#Iterate through the urls, and only return the ones that contain course and --
links = []
for u in url:
    if (u['href'].find('course') != -1):
        if (u['href'].find('--') != -1):
            if (u['href'].find('nanodegree') == -1):
                links.append(u['href'])
            
#remove duplicates by converting the list of links to a set, then back to a list
links = list(set(links)) 
links

len(links)

"""There are 200 unique links that belong to the different courses on Udacity. These are the courses that can be done without enrolling in a nanodegree.

Each course URL will be visited in the following code to extract the information needed. The relevent HTML tags and class names were extracted by visiting the URL, right clicking on the element of interest and selecting Inspect to view the related HTML. This could then be used with the BeautifulSoup find and find_all functions to get the information.
"""

import time
from random import randint

#initilise the lists to store the extracted info
course_names = []
course_urls = []
course_nanodegrees = []
course_costs = []

# Preparing the monitoring of the loop
start_time = time.time()
requests_count = 0

#iterate through each link to extract info about the nano degree it belongs to and the cost
for i in range(len(links)):
    #need to make requests at random times in order to not be blocked by website
    time.sleep(randint(2,6)) 
    url = "https://www.udacity.com" + links[i]
    response = requests.get(url)
    content = response.content
    html_soup = BeautifulSoup(content, 'html.parser')
    
    # Monitor the requests
    requests_count += 1
    elapsed_time = time.time() - start_time
    print('Request:{}; Frequency: {} requests/s'.format(requests_count, requests_count/elapsed_time))
    #clear_output(wait = True)
    
    # Break the loop if the number of requests is greater than expected
    if requests_count > 250:
        warn('Number of requests was greater than expected.')  
        break 
    
    try:
        nanodegree = html_soup.find('h3',class_="h1 hero__nanodegree--title").text
    except:
        nanodegree = 'None'
    try:
        course_name = html_soup.find('h1',class_='hero__course--title').text
    except:
        course_name = 'None'
    try:
        cost = html_soup.find('h6',class_='hero__course--type').text
    except:
        cost = 'None'
    course_names.append(course_name)
    course_urls.append(url)
    course_nanodegrees.append(nanodegree)
    course_costs.append(cost)

#Convert the lists to a dataframe
import pandas as pd
courses_df = pd.DataFrame({'Name of course': course_names,
                          'URL': course_urls,
                          'Nanodegree': course_nanodegrees,
                          'Cost':course_costs})
courses_df = courses_df.sort_values('Nanodegree')
print(courses_df.info())
courses_df

#Save to csv file
courses_df.to_csv('Udacity_Courses.csv')



